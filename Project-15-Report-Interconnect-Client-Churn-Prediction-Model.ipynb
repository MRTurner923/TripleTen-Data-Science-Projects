{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: solid blue 2px; padding: 15px; margin: 10px\">\n",
    "  <b>Overall Summary of the Project ‚Äì Final Report</b><br><br>\n",
    "\n",
    "  Hi Michael, I‚Äôm <b>Victor Camargo</b> (<a href=\"https://hub.tripleten.com/u/e9cc9c11\" target=\"_blank\">TripleTen Hub profile</a>). Thanks for your detailed submission, I‚Äôve reviewed your final project and report and I'm happy to say it is now <b>approved</b>! üéâ<br><br>\n",
    "\n",
    "  <b>Nice work on:</b><br>\n",
    "  ‚úîÔ∏è Merging and cleaning all four datasets with consistent column names and correct handling of missing values.<br>\n",
    "  ‚úîÔ∏è Converting data types properly and engineering meaningful features such as <b>tenure_days</b> and <b>tenure_months</b>.<br>\n",
    "  ‚úîÔ∏è Creating a <b>DummyClassifier</b> baseline and then testing a wide range of models (Logistic Regression, Decision Tree, Random Forest, XGBoost, LightGBM, CatBoost).<br>\n",
    "  ‚úîÔ∏è Addressing the <b>class imbalance</b> thoughtfully using weights and <code>scale_pos_weight</code> in boosted models.<br>\n",
    "  ‚úîÔ∏è Implementing <b>Pipelines</b> with preprocessing, which keeps the workflow clean and reproducible.<br>\n",
    "  ‚úîÔ∏è Using both validation and cross-validation with the ROC-AUC metric, and then confirming performance on the holdout test set.<br>\n",
    "  ‚úîÔ∏è Writing a clear conclusion, with strong reasoning for selecting <b>CatBoost</b> as the final model based on both performance and ease of handling categorical features.<br><br>\n",
    "\n",
    "  <b>Final note:</b><br>\n",
    "  You‚Äôve built a very solid end-to-end machine learning pipeline. The structure of your work, the clarity of your results, and the quality of your reflection all show excellent understanding. <b>Congratulations on reaching this milestone ‚Äî great job!</b> üöÄüí™\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Model Testing and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_df = pd.read_csv('/datasets/final_provider/contract.csv')\n",
    "personal_df = pd.read_csv('/datasets/final_provider/personal.csv')\n",
    "internet_df = pd.read_csv('/datasets/final_provider/internet.csv')\n",
    "phone_df = pd.read_csv('/datasets/final_provider/phone.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_df = contract_df.rename(columns = {'customerID':'customer_id', 'BeginDate':'begin_date', 'EndDate':'end_date',\n",
    "                                            'Type':'type', 'PaperlessBilling':'paperless_billing', 'PaymentMethod':'payment_method',\n",
    "                                           'MonthlyCharges':'monthly_charges', 'TotalCharges':'total_charges'})\n",
    "personal_df = personal_df.rename(columns = {'customerID':'customer_id', 'SeniorCitizen':'senior_citizen', 'Partner':'partner',\n",
    "                                           'Dependents':'dependents'})\n",
    "internet_df = internet_df.rename(columns = {'customerID':'customer_id', 'InternetService':'internet_service', \n",
    "                                            'OnlineSecurity':'online_security', 'OnlineBackup':'online_backup',\n",
    "                                           'DeviceProtection':'device_protection', 'TechSupport':'tech_support',\n",
    "                                           'StreamingTV':'streaming_tv', 'StreamingMovies':'streaming_movies'})\n",
    "phone_df = phone_df.rename(columns = {'customerID':'customer_id', 'MultipleLines':'multiple_lines'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge1 = contract_df.merge(personal_df, on = 'customer_id', how = 'left')\n",
    "merge2 = internet_df.merge(phone_df, on = 'customer_id', how = 'left')\n",
    "df = merge1.merge(merge2, on = 'customer_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet_columns = ['internet_service', 'online_security', 'online_backup', 'device_protection', 'tech_support', 'streaming_tv',\n",
    "                   'streaming_movies']\n",
    "df[internet_columns] = df[internet_columns].fillna('No internet service')\n",
    "\n",
    "df['multiple_lines'] = df['multiple_lines'].fillna('No phone service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_charges'] = pd.to_numeric(df['total_charges'], errors = 'coerce')\n",
    "df['total_charges'] = df['total_charges'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['begin_date'] = pd.to_datetime(df['begin_date'])\n",
    "df['end_date'] = pd.to_datetime(df['end_date'].replace(\"No\", pd.NaT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['partner'] = df['partner'].replace('Yes', 1)\n",
    "df['partner'] = df['partner'].replace('No', 0)\n",
    "df['paperless_billing'] = df['paperless_billing'].replace('Yes', 1)\n",
    "df['paperless_billing'] = df['paperless_billing'].replace('No', 0)\n",
    "df['dependents'] = df['dependents'].replace('Yes', 1)\n",
    "df['dependents'] = df['dependents'].replace('No', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = pd.to_datetime(\"2020-02-01\")\n",
    "df['tenure_days'] = (df['end_date'].fillna(cutoff_date) - df['begin_date']).dt.days\n",
    "df['tenure_months'] = df['tenure_days'] // 30\n",
    "df['churn'] = df['end_date'].notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7043, 19) (7043,)\n"
     ]
    }
   ],
   "source": [
    "customer_ids = df['customer_id']\n",
    "features = df.drop(['customer_id', 'begin_date', 'end_date', 'churn'], axis = 1)\n",
    "target = df['churn']\n",
    "print(features.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = features.select_dtypes(include = ['object']).columns\n",
    "num_cols = ['monthly_charges', 'total_charges', 'tenure_months', 'tenure_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4225, 19) (4225,)\n",
      "\n",
      "(1409, 19) (1409,) (1409, 19) (1409,)\n"
     ]
    }
   ],
   "source": [
    "features_train, features_temp, target_train, target_temp = train_test_split(features, target, test_size = 0.4, random_state = 2356)\n",
    "features_test, features_valid, target_test, target_valid = train_test_split(features_temp, target_temp, test_size = 0.5, random_state = 2356)\n",
    "print(features_train.shape, target_train.shape)\n",
    "print()\n",
    "print(features_valid.shape, target_valid.shape, features_test.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting our data into a training, test, and validation set at a typical 3:1:1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [('num', StandardScaler(), num_cols), ('cat', OneHotEncoder(drop = 'first'), cat_cols)\n",
    "                   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5174\n",
       "True     1869\n",
       "Name: end_date, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#determining class weights and counts for scale_pos_weight use in XGB and LGBM\n",
    "class_counts = df['end_date'].notna().value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = class_counts[0] / class_counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_boost_weights = [(len(df) / (2 * class_counts[0])), (len(df) / (2 * class_counts[1]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Evaluation of Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.892073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.890113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.886889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.854115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.845545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.724422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ROC-AUC\n",
       "Model                        \n",
       "LightGBM             0.892073\n",
       "XGBoost              0.890113\n",
       "CatBoost             0.886889\n",
       "Random Forest        0.854115\n",
       "Logistic Regression  0.845545\n",
       "Decision Tree        0.724422"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {'Logistic Regression': LogisticRegression(class_weight = 'balanced',\n",
    "                                                                    solver = 'liblinear',\n",
    "                                                                    random_state = 2356\n",
    "                                                   ),\n",
    "          'Decision Tree': DecisionTreeClassifier(class_weight = 'balanced',\n",
    "                                                  random_state = 2356\n",
    "                                                 ),\n",
    "          'Random Forest': RandomForestClassifier(class_weight = 'balanced',\n",
    "                                                  random_state = 2356\n",
    "                                                 ),\n",
    "          'XGBoost': XGBClassifier(scale_pos_weight = scale_pos_weight,\n",
    "                                   use_label_encoder = False,\n",
    "                                   eval_metric = 'auc',\n",
    "                                   random_state = 2356\n",
    "                                  ),\n",
    "          'LightGBM': LGBMClassifier(scale_pos_weight = scale_pos_weight,\n",
    "                                     random_state = 2356\n",
    "                                    ),\n",
    "          'CatBoost': CatBoostClassifier(class_weights = cat_boost_weights,\n",
    "                                         random_state = 2356,\n",
    "                                         verbose = 0\n",
    "                                        )\n",
    "         }\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor), ('classifier', model)\n",
    "    ])\n",
    "    pipeline.fit(features_train, target_train)\n",
    "    pred_prob = pipeline.predict_proba(features_valid)[:, 1]\n",
    "    auc = roc_auc_score(target_valid, pred_prob)\n",
    "    results[name] = auc\n",
    "    results_df = pd.DataFrame.from_dict(results, orient = 'index', columns = ['ROC-AUC'])\n",
    "    results_df.index.name = 'Model'\n",
    "    results_df = results_df.sort_values('ROC-AUC', ascending = False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the ROC-AUC scores of our models before cross-validation gives me a good idea of results to expect after CV. The models might not perform the exact same but this at least helps me understand which models are separating themselves from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_auc</th>\n",
       "      <th>std_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.885597</td>\n",
       "      <td>0.007435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.883571</td>\n",
       "      <td>0.009063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.883344</td>\n",
       "      <td>0.007506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.851195</td>\n",
       "      <td>0.014797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.846320</td>\n",
       "      <td>0.013522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.696199</td>\n",
       "      <td>0.014131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     mean_auc   std_auc\n",
       "XGBoost              0.885597  0.007435\n",
       "CatBoost             0.883571  0.009063\n",
       "LightGBM             0.883344  0.007506\n",
       "Logistic Regression  0.851195  0.014797\n",
       "Random Forest        0.846320  0.013522\n",
       "Decision Tree        0.696199  0.014131"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {'Logistic Regression': LogisticRegression(class_weight = 'balanced',\n",
    "                                                                    solver = 'liblinear',\n",
    "                                                                    random_state = 2356\n",
    "                                                   ),\n",
    "          'Decision Tree': DecisionTreeClassifier(class_weight = 'balanced',\n",
    "                                                  random_state = 2356\n",
    "                                                 ),\n",
    "          'Random Forest': RandomForestClassifier(class_weight = 'balanced',\n",
    "                                                  random_state = 2356\n",
    "                                                 ),\n",
    "          'XGBoost': XGBClassifier(scale_pos_weight = scale_pos_weight,\n",
    "                                   use_label_encoder = False,\n",
    "                                   eval_metric = 'auc',\n",
    "                                   random_state = 2356\n",
    "                                  ),\n",
    "          'LightGBM': LGBMClassifier(scale_pos_weight = scale_pos_weight,\n",
    "                                     random_state = 2356\n",
    "                                    ),\n",
    "          'CatBoost': CatBoostClassifier(class_weights = cat_boost_weights,\n",
    "                                         random_state = 2356,\n",
    "                                         verbose = 0\n",
    "                                        )\n",
    "         }\n",
    "cross_val_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor), ('classifier', model)\n",
    "    ])\n",
    "    scores = cross_val_score(\n",
    "        pipeline,\n",
    "        features_train,\n",
    "        target_train,\n",
    "        cv = 5,\n",
    "        scoring = 'roc_auc'\n",
    "    )\n",
    "\n",
    "    cross_val_results[name] = {\n",
    "        'mean_auc': scores.mean(),\n",
    "        'std_auc': scores.std()\n",
    "    }\n",
    "cross_val_df = pd.DataFrame(cross_val_results).T\n",
    "cross_val_df = cross_val_df[['mean_auc', 'std_auc']]\n",
    "cross_val_df = cross_val_df.sort_values('mean_auc', ascending = False)\n",
    "cross_val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running cross-validation on the chosen models, the boosted models still performed the best, all showing mean ROC-AUC scores just over 0.88 and very minimal standard deviations. The simpler models like Logistic Regression and Random Forest performed well, and the Decision Tree Classifier was the poorest of the bunch, not even topping 0.7. For evaluating with the test set, I'll be using the 3 boosted models and comparing their performance against each other and the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Analysis of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.867685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.866738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.864549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ROC-AUC\n",
       "Model             \n",
       "CatBoost  0.867685\n",
       "LightGBM  0.866738\n",
       "XGBoost   0.864549"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {'XGBoost': XGBClassifier(scale_pos_weight = scale_pos_weight,\n",
    "                                   use_label_encoder = False,\n",
    "                                   eval_metric = 'auc',\n",
    "                                   random_state = 2356\n",
    "                                  ),\n",
    "          'LightGBM': LGBMClassifier(scale_pos_weight = scale_pos_weight,\n",
    "                                     random_state = 2356\n",
    "                                    ),\n",
    "          'CatBoost': CatBoostClassifier(class_weights = cat_boost_weights,\n",
    "                                         random_state = 2356,\n",
    "                                         verbose = 0\n",
    "                                        )\n",
    "         }\n",
    "test_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor), ('classifier', model)\n",
    "    ])\n",
    "    pipeline.fit(features_train, target_train)\n",
    "    pred_prob = pipeline.predict_proba(features_test)[:, 1]\n",
    "    auc = roc_auc_score(target_test, pred_prob)\n",
    "    test_results[name] = auc\n",
    "    test_results_df = pd.DataFrame.from_dict(test_results, orient = 'index', columns = ['ROC-AUC'])\n",
    "    test_results_df.index.name = 'Model'\n",
    "    test_results_df = test_results_df.sort_values('ROC-AUC', ascending = False)\n",
    "test_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After letting the models analyze the test data, the results are very promising! The CatBoost model gets a slight edge over LGBM, with XGB not too far behind. Knowing that a score closer to 1 is indicative of a stronger model, we have 3 very high quality choices on our hands with these boosted models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back at my work plan (Feature Engineering/Encoding -> Scaling and Splitting Data -> Creating Baseline and Test Models -> Model Evaluation and Selection -> Conclusion), I performed all of these steps to the best of my ability. There wasn't as much feature engineering as I initially thought, and since I'd already completed most of it in my work plan notebook, I was able to finish that step and the data scaling and splitting very quickly. As expected, most of the work and troubleshooting after came during model development and testing. Luckily we'd used these models for various other projects in the past so I was able to reference those and my notes to work through whatever issues I faced. I initially started each model separately, then after I was able to find scores for each, I was able to build the loops that tested each model at once and kept things much more organized. The Pipeline feature was new to me so I relied on some Google searching and documentation to help answer my questions about using that, and it saved me a ton of work and frustration I remember from testing models in older projects! Pipeline was definitely one of my key steps, in addition to the tips from the tutors about focusing on boosted models and how to treat their class weights after determining an imbalance earlier. \n",
    "\n",
    "In conclusion, I would recommend the CatBoost model as it showed the highest ROC-AUC score on the unseen test data coming in at 0.867. Slightly edging out LightGBM at 0.866, it is marginally better and if we were to add further categorical data to the set, it would be easier to interpret as we would not need to encode that data like we would for LightGBM models, saving us some time when taking the analysis even further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
